# [RLCD: Reinforcement Learning From Contrastive Distillation For LM Alignment](https://arxiv.org/abs/2307.12950)

## Summary

Proposes an improvement over RLAIF called RLCD, in which instead of creating preference pairs for RLAIF by scoring multiple samples from the same prompted LM, a positive and negative example are created using a differently prompted LM from which typical RL or DPO can be performed.

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms 

- RLAIF
- Context Distillation
- Synthetic Preference Pair Generation

## Benchmarks

Tasks evaluated.

- Harmlessness after RLHF on the synthetic data (HH-RLHF harmlessness prompts)
- Helpfulness after RLHF on the synthetic data (HH-RLHF helpfulness prompts)
- Story outline generation

- Win-rates according to GPT-4
- Accuracy of resulting preference models

## Metric Results

- Primarily shares fine-tuning results (and GPT-4 winrates of fine-tuned model vs. baselines).
- Measures quality (Table 6) of labels applied to the paired dataset by checking accuracy of GPT-4 in relabeling the preference pairs blindly.

## Paper Tags

1. SYNTH
2. JUDGE

## Other comments

- Uses PPO for fine-tuning. Appendix has notes on hyperparameter selection
- Similar to Pink Elephants in the *directed* creation of preference pairs to provide increased signal via meaningfully contrasting instances.
