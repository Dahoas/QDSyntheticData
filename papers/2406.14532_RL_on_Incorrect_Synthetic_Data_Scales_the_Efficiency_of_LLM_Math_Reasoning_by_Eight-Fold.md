# [RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold](https://arxiv.org/abs/2406.14532)

ID: 316

## Summary

The paper finds that not all types of positive synthetic data are equally effective; positive responses self-generated by the learner are as effective as 2× synthetic data from bigger models. Also proposes a method to construct learner-specific negative data with emphasis on critical steps (steps in the solution trace after which the model is unable to recover) which results in a performance boost equivalent to scaling up positive data 8×.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

- Per-Step DPO

- Per-Step Credit Assignment with Negative Synthetic Data

## Benchmarks

- GSM8K

- MATH

## Metric Results

- On positive data, training on self-generated positives results in almost the same test error of training on 2x samples generated from larger model, on both GSM8K and MATH.

- On negative data, step-level RL with negative data results in almost the same test errors of 8x positive synthetic data.

## Paper Tags

Tag paper with relevant categories. See [here](https://github.com/Dahoas/QDSyntheticData/blob/main/papers/categories.json) for list of all category tags.

1. DVFT
2. SYNTH
3. REASONING
4. DIVERSITY

## Relevant related work

- Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, and Minjoon Seo. **Self-explore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards**. [Link](https://arxiv.org/abs/2404.10346)

## Other comments

- The paper also offers a conceptual model to explain the results, and shows that the construction negative data with emphasis on “critical” tokens is equivalent to training the model with per-step advantage-weighted reinforcement learning.
