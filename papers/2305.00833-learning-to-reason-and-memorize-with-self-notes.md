# [Learning to Reason and Memorize with Self-Notes](https://arxiv.org/abs/2305.00833)

## Summary

Typically other strategies like Chain of Thought start generating text at the end of the prompt (post-context thoughts).
Self Notes allow an LLM to generate text before the end of the prompt (in-context thoughts), augmenting the prompt itself.
It maintains the same vocabulary, and selects a set of token to be "trigger" tokens, when these tokens are present in the prompt a generation phase starts.
Some of the results seems very promising particularly for the Algorithmic tests.

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms

- ( early ) sampling

## Benchmarks

Tasks evaluated.

- ToyStory: babi-like dataset of stories for reasoning
- Algorithmic:
- Boolean Variable:
- Chess Piece Type:
- Chess Move:

## Metric Results

- How is quality measured? Few shots prompting accuracy

## Other comments

Interesting the improvement on the Algorithmic dataset compared to CoT (from 72.2 to 95.5)
