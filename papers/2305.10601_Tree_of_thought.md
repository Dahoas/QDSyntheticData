# [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)

Parallel paper: [Large Language Model Guided Tree-of-Thought](https://arxiv.org/abs/2305.08291)

ID: 131

## Summary

Introduces a new framework for language model inference, “Tree of Thoughts” (ToT). 
It generalizes over “Chain of Thought” prompting, and enables
exploration over coherent units of text (“thoughts”) that serve as intermediate steps
toward problem solving. 
ToT allows LMs to perform deliberate decision making
by considering multiple different reasoning paths and self-evaluating choices to
decide the next course of action, as well as looking ahead or backtracking when
necessary to make global choices

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms

- sampling

## Benchmarks

ToT enhanced LMs problem-solving abilities on three novel tasks requiring
non-trivial planning or search: 

- Game of 24
- Creative Writing
- Mini Crosswords

These tasks require deductive, mathematical, commonsense, lexical reasoning abilities,
and a way to incorporate systematic planning or search.

Note: 

* Game of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic
arithmetic operations (+-*/) to obtain 24. For example, given input “4 9 10 13”, a solution output
could be “(10 - 4) * (13 - 9) = 24”.
* Creative writing task input is 4 random sentences and the output should
be a coherent passage with 4 paragraphs that end in the 4 input sentences respectively. Such a task is
open-ended and exploratory, and challenges creative thinking as well as high-level planning.
* Cross words explores 5 × 5 mini crosswords as a harder search problem involving
natural language. It aims to explore the limit of LM as a general problem solver that explores its own thoughts
and guides its own exploration with deliberate reasoning as heuristics. 

## Metric Results

For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only
solved 4% of tasks, Their ToT method achieved a success rate of 74%.

## Paper Tags

1. REASONING
2. SEARCH

## Other comments

Aims to address two key
shortcomings of existing approaches that use LMs to solve general problems: 1) Locally, they do not
explore different continuations within a thought process – the branches of the tree. 2) Globally, they
do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different
options – the kind of heuristic-guided search that seems characteristic of human problem-solving. 

 A specific instantiation of ToT involves answering four questions:
1. How to decompose the intermediate process into thought steps; 2. How to generate potential
thoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.

1. Thought decomposition: In general, a thought
should be “small” enough so that LMs can generate promising and diverse samples (e.g. generating
a whole book is usually too “big” to be coherent), yet “big” enough so that LMs can evaluate its
prospect toward problem solving (e.g. generating one token is usually too “small” to evaluate).

2. Thought generator: Considers two strategies to
generate k candidates for the next thought step. (a) Sampling i.i.d. thoughts from a CoT prompt, this works better when the thought
space is rich (e.g. each thought is a paragraph), and i.i.d. samples lead to diversity. (b) Propose thoughts sequentially using a
“propose prompt”, which orks better when the thought
space is more constrained (e.g. each thought is just a word or a line), so proposing different
thoughts in the same context avoids duplication.

3. State evaluator: evaluates the
progress they make towards solving the problem, serving as a heuristic for the search algorithm
to determine which states to keep exploring and in which order. These are typically either programmed (e.g. DeepBlue) or
learned (e.g. AlphaGo). But they propose a third alternative, by using the LM to deliberately reason
about states. They either value each state independently, and/or vote across states.

4. Search algorithm: They explore two simple search algorithms (a) breadth-first search (BFS) and (b) Depth-first search (DFS). 
