# [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)

## Summary

We continue the investigation into the power of smaller Transformer-based language models as initiated by TinyStories a 10 million parameter model that can produce coherent English and the follow-up work on phi-1, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art.

The latter work proposed to use existing Large Language Models (LLMs) to generate "textbook quality" data as a way to enhance the learning process compared to traditional web data.

phi-1.5:
- 1.3B 
- 30B tokens (mostly synthetic data).
- random initialization with constant learning rate 2eâˆ’4 (no warmup) 
- weight decay 0.1
- Adam 0.9, 0.98, eps=1e-7
- fp16 with Zero-2
- batch size 2048
- 150B tokens (Wasn't 30B?)


The creation of a robust and comprehensive dataset demands more than raw computational power: It requires:
- intricate iterations
- strategic topic selection
- deep understanding of knowledge gaps to ensure quality and diversity of the data.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

Generates ~ 20B tokens by carefully selected 20K topics to seed the generation of this new synthetic data
In our generation prompts, we use samples from web datasets for diversity. 7B tokens of code filtered data.

## Benchmarks

Common Sense:
- WinoGrande 
- ARC-Easy 
- ARC-Challenge 
- BoolQ 
- SIQA

Language Understanding:
- PIQA
- Hellaswag
- OpenBook QA
- MMLU (2-shot)
- SQUAD

Reasoning:
- GSM8K
- Humaneval
- MBPP

## Metric Results

Standard dataset evaluation

## Paper Tags

1. MODEL

## Other comments

The main insight of this paper is that the phi models are trained mostly on synthetically generated dataset.

## Questions

- How small a LLM can be to achieve a specific task? i.e. Fluent English, Python Coding.
