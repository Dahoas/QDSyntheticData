# [Aligning Large Language Models through Synthetic Feedback](https://arxiv.org/abs/2305.13735)

## Summary

This paper introduces a variant of RLHF where virtually every step of human annotation is replaced by synthetic data generation, using pre-trained LLMs of different sizes and with different few-shot (in-context learning) prompts. First, ranking data is generated, using the assumption that responses are better if they come from larger models / higher number of shots / better few-shot examples; some automatic filtering is done, to reduce the amount of incorrectly-ranked pairs. The ranking data is used to train a reward model. Then, SFT data is obtained by generating several responses and performing rejection sampling with the RM. The SFT and RL steps of RLHF follow.


## Relevance to survey topic (1-5)

Relevance: 5

## Algorithms

- Prompt LLMs with in-context learning to generate ranking data (making assumptions on which responses shall be better) and SFT data (with rejection sampling).

## Benchmarks

- Static HHH alignment
- TruthfulQA
- Vicuna Questions (with human evaluation and GPT-4 evaluation)


## Metric Results

- The final SFT and RLHF-trained models (based off LLaMA-7B) are compared against Dolly-v2-7B, Alpaca-7B, and Vicuna-7B, on the above benchmarks. The proposed models are found to be better than Dolly-v2-7B and Alpaca-7B, but worse than Vicuna-7B.
- The RM is also evaluated.
- Data quality and diversity are not directly evaluated.


## Other comments

While quality and diversity are not directly evaluated, this paper introduces a relevant method to generate synthetic ranking data and SFT data.