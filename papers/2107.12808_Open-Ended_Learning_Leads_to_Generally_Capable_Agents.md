# [Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808)

[blog post](https://deepmind.google/discover/blog/generally-capable-agents-emerge-from-open-ended-play/)

ID: 115

## Summary

Deepmind's first step (July '21) to training an agent capable of playing many different games without needing human interaction data, 
resulting in zero-shot generalisation from RL agents.

They trained an open-ended agent with RL on a game environment called the XLand space (a consistent, human-relatable 3D world). 
This simulated space allows for procedurally generated tasks so they could generate billions of tasks in XLand, across varied games (many multiplayer games), worlds, and players.
The agent’s capabilities improve iteratively as a response to the challenges that arise in training, with the learning process continually refining the training tasks so the agent never stops learning.
They develop new training algorithms that support the open-ended creation of complexity. 

## Relevance to survey topic (1-5)

Relevance: 3 - synthetic data for RL, doesn't involve LLMs.

## Benchmarks and metrics

To measure how agents perform within this vast universe, they create a set of evaluation tasks using games and worlds that remain separate from the training data. 
These "held-out" tasks include specifically human-designed tasks like hide and seek and capture the flag.
Evaluating the performance of agents was a challenge as each task involves different levels of complexity averaging reward doesn't make sense. 
They took a more complex approach involving normalising scores per task using the Nash equilibrium value computed using the current set of trained players, 
and took into account the entire distribution of percentiles of normalised scores. 

## Paper Tags

SYNTH

## Other comments

Data generation note:

XLand consists of a galaxy of games, with each game able to be played in many different simulated worlds whose topology and characteristics vary smoothly. 
An instance of an XLand task combines a game with a world and co-players.
Because XLand can be programmatically specified, the game space allows for data to be generated in an automated and algorithmic fashion. 
Every task is generated to be neither too hard nor too easy, but just right for training.
And because the tasks in XLand involve multiple players, the behaviour of co-players greatly influences the challenges faced by the AI agent. 
These complex, non-linear interactions create an ideal source of data to train on, since sometimes even small changes in the components of the 
environment can result in large changes in the challenges for the agents.

The games: 

The games range from simple to complex, and can be cooperative or competitive. 
Goals are communicated in language. 
Some games are as simple as cooperative games to find objects and navigate worlds, where the goal for a player could be "be near the purple cube." 
More complex games can be based on choosing from multiple rewarding options, such as "be near the purple cube or put the yellow sphere on the red floor".
More competitive games include playing symmetric hide and seek where each player has the goal, "see the opponent and make the opponent not see me." 
Each game defines the rewards for the players, and each player’s ultimate objective is to maximise the rewards.

Training: 

The players sense their surroundings by observing RGB images and receive a text description of their goal, and they train on a range of games. 
Training is done with deep RL at the core updating the neural networks of agents with every step of experience
The neural network architecture uses an attention mechanism over the agent’s internal recurrent state. 
(This helps guide the agent’s attention with estimates of subgoals unique to the game the agent is playing. 
They found this goal-attentive agent (GOAT) learns more generally capable policies.)
They use population based training (PBT) to adjust the parameters of the dynamic task generation based on a fitness that aims to improve agents’ general capability. 
And finally we chain together multiple training runs so each generation of agents can bootstrap off the previous generation.
They trained agents for 5 generations. 
Playing roughly 700,000 unique games in 4,000 unique worlds within XLand, each agent in the final generation experienced 200 billion training steps as a result of 3.4 million unique tasks. 

Zero-shot generalisation: 

As a result, their agents have been able to participate in every procedurally generated evaluation task, exhibiting general, zero-shot behaviour across the task space. 
Instead of agents knowing exactly the "best thing" to do in a new situation, there is evidence of agents experimenting and changing the state of the world until they’ve 
achieved a rewarding state. Agents sometimes rely on the use of other tools, including objects to occlude visibility, to create ramps, and to retrieve other objects. 
They show that with just 30 minutes of focused training on a newly presented complex task, the agents can quickly adapt, whereas agents trained with RL from scratch 
cannot learn these tasks at all.
