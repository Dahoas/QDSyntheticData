# [Textbooks Are All You Need (Phi-1)](https://arxiv.org/abs/2306.11644)

[Huggingface](https://huggingface.co/microsoft/phi-1)

[OpenReview](https://openreview.net/forum?id=Fq8tKtjACC)

ID: 135 

## Summary

The paper demonstrates the remarkable impact of high-quality data in honing a
language model’s proficiency in code-generation task. 
Introduces phi-1, a 1.3B parameter code model, for particularly for converting python docstrings to functions. 
The model was trained for 4 days on 8 A100s on 6B tokens of "textbook quality" data, 1B tokens of GPT3.5 synthetically generated textbooks and exercises. 
Obtains accuracy on HumanEval and MBPP comparable with much larger models. 
The goal of the paper is to explore the improvement from improving data quality. 
## Relevance to survey topic (1-5)

Relevance: 4 

## Benchmarks


- Humaneval (pass@1) 50.6%
- MBPP (pass@1) 55.5% - Mostly Basic Python Programs

## Metric Results

- How is quality measured? The code-language dataset (The stack and StackOverflow) is filtered using a LM based classifier (see other comments). 
- How is diversity measured? Not measured, but synthetic data generation process constrains topics and difficulty level to promote diversity. 
- Fine-tuning results? Pretrained and fine-tuned model results are both impressivefor small models (See figure 2.1). 

## Paper Tags

1. SYNTH
2. MOTIV
3. QUALITY

## Other comments

### model details
Phi-1 has 24 layers, hidden dimension 2048. 
Phi-1-small is 350M parameters and there are some results for this model too. 

### Training data details
They pretrain roughly 8 passes over 7B tokens (slightly over 50B total tokens seen) followed by finetuning on less than
200M tokens. So they pretrain on “textbook quality” data, both synthetically generated
(with GPT-3.5) and filtered from web sources, and we finetune on “textbook-exercise-like” data.
Previous works train on The Stack (which contains sourcecode from repositories with permissive licenses) and other
web-based datasets (e.g., StackOverflow and CodeContest) which are noisy, ambiguous, and incomplete, and so suboptimal to learn from. 

Training relies on three main datasets (altogether ~7B tokens):

* For pretraining: A filtered code-language dataset, which is a subset of The Stack and StackOverflow, obtained by
using a language model-based classifier (consisting of about 6B tokens). See (1).

* For pretraining: A synthetic textbook dataset consisting of <1B tokens of GPT-3.5 generated Python textbooks. See (2).

* For fine-tuning: A small synthetic exercises dataset consisting of ∼180M tokens of Python exercises and solutions. See (3).

(1) Obtained from a python subset of the deduplicated version of The Stack and the StackOverflow (35 million samples or >35B tokens). 
They *annotate the quality of about 100k samples* using GPT-4 (uses GPT4 minimally only for annotations, 3.5 for rest): given a code snippet, the model is prompted to “determine its educational value for a student
whose goal is to learn basic coding concepts”.
They then use this annotated dataset to train a *random forest classifier* that predicts the quality of
a file/sample using its output embedding from a pretrained codegen model as features. 

(2) They wanted to ensure that the
examples are diverse and non-repetitive. By diversity, they mean that the examples should cover a wide
range of coding concepts, skills, and scenarios, and that they should vary in their level of difficulty,
complexity, and style. 
They generated less than 1B tokens of GPT-3.5 generated Python textbooks. 
Diversity is obtained by providing constraints on topics and target audience of
the generated textbook.

(3) GPT-3.5 generated. The main means of eliciting diversity was by constraining
the function names. For this dataset in particular, they conducted explicit decontamination and alternative
evaluations to ensure that problems similar to those from HumanEval benchmark are not seen during fine-tuning 
(though it was still rejected from ICLR over contamination concerns).


