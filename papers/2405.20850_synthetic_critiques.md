# [Improving Reward Models with Synthetic Critiques](https://arxiv.org/abs/2405.20850)

## Summary: 

RMs overfit on superficial features in the training set, hindering their generalization performance on unseen distributions.
Paper uses synthetic natural language critiques generated by LLMs to provide additional feedback, evaluating aspects such as instruction following, correctness, and style. 
First prompt LLMs to generate critiques individually for the chosen and rejected part of a preference example, and they then train a RM that predicts a scalar reward on top of them. 

The critiques break down both positive and negative features of the completion, evaluating it based on how effectively it fulfills the prompt requirements in aspects such as instruction-following, truthfulness, and helpfulness.

Shows that high-quality critiques improve the performance and data efficiency of RMs initialized from different pretrained models and that low-quality critiques negatively impact performance.
Once we have the critiques generated from LLMs, they augment the training data by by concatenating the critiques after each completion to form new preference pairs, which become a critique-augmented training set. 

## Results

Results evaluate on RewardBench in table 2. In most cases, adding critiques improves RM test accuracy compared to No-Critiques baseline. 

## Notes
Adding as a possibly relevant paper. 
