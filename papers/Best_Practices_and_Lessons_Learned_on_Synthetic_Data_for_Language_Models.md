# [Best Practices and Lessons Learned on Synthetic Data for Language Models](https://arxiv.org/abs/2404.07503)


## Summary

Summarizies AI domains utlizing synthetic data. These include math/code/other reasoning tasks, tool use and planning, multi-linguality, and instruction following.

## Relevance to survey topic (1-5)

Relevance: 2


## Paper Tags

Tag paper with relevant categories. See [here](https://github.com/Dahoas/QDSyntheticData/blob/main/papers/categories.json) for list of all category tags.

1. SURVEY

## Relevant related work

"Researchers have also explored different sampling
methods for back-translation (e.g., beam search, constrained sampling, unconstrained sampling) and
their comparative effectiveness (Edunov et al., 2018; Gra√ßa et al., 2019; Sennrich et al., 2016). Xu
et al. (2022) emphasize the importance of the weight and quality of synthetic data for optimal NMT
performance using back-translation. They propose a method to optimize the ratio between search
methods and a gamma score to balance estimated importance weight and quality. However, some
limitations exist with back-translation-based synthetic data generation. For example, the quality
and diversity of synthetic data depends on the performance of the back-translation method. If the
synthetic data is too noisy or not diverse, the performance gain would be limited (Chauhan et al.,
2022; Epaliyana et al., 2021)"

Suggests we should look into translation/backtranslation literature more closely.
