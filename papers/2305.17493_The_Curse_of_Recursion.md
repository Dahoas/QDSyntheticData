# [THE CURSE OF RECURSION: TRAINING ON GENERATED DATA MAKES MODELS FORGET](https://arxiv.org/abs/2305.17493)

## Summary

The paper provides a mathematical grounding for "Model Collapse", where generative models trained on data produced by preceding model iterations gradually lose fidelity to the original data distribution (esp. the tail).  This leads to a degradation in model performance and an increase in the generation of erroneous outputs (compounding with each successive generation), reflecting a drift from the original data distribution.

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms

- Continual Learning
- Sequential Data Sampling

## Benchmarks

Tasks evaluated.

- Perplexity on original wikitext2 test

## Metric Results

- quality: perplexity increases with successive model generations.
- diversity: models generations track closely to the original data distribution (and produce more sequences that the original model would produce with the higher likelihood), but over generations, they develop longer tails, indicating an accumulation of errors.
- Fine-tuning results: a portion of the original data during fine-tuning leads to a slower degradation in performance.
- removing repeated phrases from the synthetic dataset (sampling with penalty) leads to a performance degradation

## Paper Tags

1. SYNTH
2. DIVERSITY
3. QUALITY

## Other comments

Anything else you found interesting/noteworthy.

## Questions

TThey focus exclusively on continual fine-tuning for a specific task. Would have been interesting to see how this generalizes to SFT.