# [NExT: Teaching Large Language Models to Reason about Code Execution](https://arxiv.org/abs/2404.14662)

## Summary

The main idea of NExT represents execution trace as inline comments. 

Finetune LLMs on high-quality CoT rationales that reason about program execution.

The execution information is limited to common information that human developers also use.

- program states
- variable values from the execution trace

NExT uses weakly supervised self-training to bootstrap a synthetic training set by sampling rationales that lead to correct task solutions verified by unit tests.

It employs expert iteration to improve a base LLM using synthetic rationales sampled from the model.

NExT executes this self-training loop for multiple iterations.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithm

1. Given a training set $D$ of repair tasks with execution traces, NExT first samples candidate NL rationales and fixed code solutions from the LLM. 
2. Those candidate solutions are filtered using unit test execution diagnostics, and those that pass all test cases are then used to update the model via fine-tuning.
3. This sample-filter-train loop is performed for multiple iterations, improving the modelâ€™s rationales and repair success rate after each iteration.

## Questions:

- Can LLMs reason with program traces in natural language?

## Datasets

- [Mbpp](https://github.com/google-research/google-research/tree/master/mbpp)
- [Human Eval](https://github.com/openai/human-eval)

## Conclusions

- Learning to reason in natural language improves generalization and sample diversity.


## Paper Tags

- SYNTH