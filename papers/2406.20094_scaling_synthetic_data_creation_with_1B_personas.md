# [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/pdf/2406.20094)

## Summary

This paper proposes a new method to generate diverse synthetic data at scale. The idea is: (1) use web data (and a prompted LLM) to generate a large number of persona descriptions (a moving company driver, a chemical kinetics researcher, a musician interested in audio processing, ...); (2) prompt an LLM to generate data from the perspective of a certain persona. Diversity in persona generation ensures diversity in data generation for virtually any task. The use case of generating math problems is analyzed with more detail, but other use cases are also discussed.

## Relevance to survey topic (1-5)

Relevance: 5

## Algorithms

- Prompt LLMs to "condense" world knowledge into several personas (say, 1B). Each persona has a detailed background, making personas quite diverse.
- Prompt LLMs to generate data from the perspective of a certain persona.

## Benchmarks

- MATH dataset

## Metric Results

- Accuracy on the MATH dataset of a 7B model fine-tuned on a 1M synthetic dataset is found to be comparable to out-of-the-box state-of-the-art LLMs such as GPT-4.
- Synthetic data diversity is measured by semantic similarity (through an OpenAI text embedding model). The similarity of generated math problems is found to be lower than the similarity between their corresponding personas, thus suggesting that synthetic data will be diverse even at a 1B scale.


## Other comments

I think this is a very cool idea to boost diversity. The idea is well justified, although I found diversity not to be thoroughly evaluated.