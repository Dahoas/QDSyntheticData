# [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/abs/2404.01413)

ID: #193 & # 225

## Summary

TLDR: Suggest that the “curse of recursion” may not be as dire as had been portrayed in previous works provided we accumulate synthetic data alongside real data, rather than replacing real data by synthetic data only.

Model collapse is a phenomenon where sequences of generative models are trained progressively on their own outputs and degrade until the latest model becomes useless (here measured by worsening error over increasing iterations). 
This paper studies model collapse under the assumption that data accumulates after each iteration, as opposed to data being replaced as supposed by previous work such as Dohmatob et al. (2024). 
They empirically study this question in a **pretraining setting**. 
They confirm that replacing the original real data with each generation’s synthetic data (fully synthetic dataset) indeed tends toward model collapse.
However, **accumulating the successive generations of synthetic data alongside the original real data avoids model collapse**. 
Some of these results are likely due to smaller dataset size for replacement scenario (see abalation experiment Table 2 in appendix).
They find these results hold on other architectures such as diffusion models for molecule conformation generation and image generation with VAEs.
Finally, they study why accumulating data can avoid model collapse using an analytical framework (from prior work) in which a sequence of linear regression
models are fit to the previous models’ outputs. 
Using this framework, they prove that if data instead accumulates, the test error has a finite upper bound independent of the number of iterations, 
meaning model collapse no longer occurs, unline in the replacement case.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

List of algorithm categories the proposed method falls under. This will take some time to formalize. Example categories include i.i.d sampling, few-shot i.i.d sampling, MAP Elites, 

- test errorL CE loss, MSE loss. 

## Benchmarks

- No benchmarks, just looks at losses.
- TinyStories dataset was used to train. 

## Metric Results

- How is quality measured? NA
- How is diversity measured? NA
- Fine-tuning results? NA

Studies model collapse by looking at error only. 

## Paper Tags

1. SYNTH 

## Relevant related work

Elvis Dohmatob, Yunzhen Feng, and Julia Kempe. Model collapse demystified: The case of
regression. arXiv preprint arXiv:2402.07712, 2024a.

<!---
## Other comments

Anything else you found interesting/noteworthy.

## Questions

Questions you had about the paper
-->
