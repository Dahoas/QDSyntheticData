# [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://openreview.net/forum?id=UnUwSIgK5W)

ID: 237

## Summary

Applies Evol-Instruct (introduced by WizardLM) to code SFT data. Streamlined evo instructions, and simplified evo prompts with unified prompt template. Adds code debugging, and code time-space complexity constraints to evo instructions

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

- Evol-Instruct (Evolution through Large Models, ELM, filters unknown?)

## Benchmarks

- Code generation benchmarks (HumanEval, HumanEval+, MBPP, DS-100, MultiPL-E)

## Metric Results

- How is quality measured?
  - N/A
- How is diversity measured?
  - N/A
- Fine-tuning results?
  - WizardCoder models close the gap compared to tested versions of ChatGPT models, doing better than open-source models
- How does dataset quality, diversity, and scale affect model generalization?
  - N/A
- How do quality, diversity, and data scale relate?
  - N/A

## Paper Tags

- SYNTH
- EVO
- JUDGE

## Other comments

- Section 5 analysis shows that complexity in data resulting from Evol-Instruct leads to better model performance after fine-tuning
- arXiv pre-print not up-to-date with ICLR version
