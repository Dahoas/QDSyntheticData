# [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)

[Huggingface](https://huggingface.co/microsoft/phi-1_5)

ID: 136

## Summary

Trains a 1.3B parameter model phi-1.5 on 30B tokens almost exclusively on “textbook-quality” synthetic data, focusing this time on common
sense reasoning in natural language. 
Performance on natural language tasks is comparable to models 5x larger, and surpassing most
non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic
coding. They claim common sense reasoning benchmark results are comparable to models ten times its size that were trained on
datasets more than ten times larger. 

## Relevance to survey topic (1-5)

Relevance: 4

## Benchmarks

Many tasks evaluated including common sense reasoning (table 2), language understanding (table 3),  reasoning with mathematics and coding (table 4). 

Phi-1.5 achieves comparable results to Llama2-7B, Falcon-7B and Vicuna-13B on nearly all of the benchmarks.

Comparison of phi-1.5-web-only trained only on filtered web data (around 15% of data mainly from Falcon) with results on Falcon-7B (trained on unfiltered version) is interesting! 

Model performs almost as well as phi-1 on code - interesting it hasn't forgot! 

They also did toxicity evaluations showing this non-instruction tuned model is less toxic than Llama2-7B and Falcon-7B. 

## Metric Results

- How is quality measured? NA. One model version is trained on filtered web, but no details are given. 
- How is diversity measured? NA they prompt for generating 20k topics. 
- Fine-tuning results? Not fine-tuned

## Paper Tags

- SYNTH
- REASONING - focus on common sense reasoning
- MOTIV

## Other comments

* Same architecture as phi-1. 
* No instruction fine-tuning or RLHF. Direct completion usage. 
* Training data: 
  - phi-1's training data (7B tokens)
  -  newly created synthetic, “textbook-like” data (roughly 20B tokens) for the purpose of teaching common sense reasoning
and general knowledge of the world (science, daily activities, theory of mind, etc.). They carefully selected
20K topics to seed the generation of this new synthetic data. In the generation prompts, they use samples
from web datasets for diversity
* Also have phi-1.5-web additionally trained on a filtered web dataset (96B tokens, 88B filtered from Falcon refined web, 7B of code data filtered from The Stack and StackOverflow)
* phi-1.5-web only is trained only on the web data (80% natural language, 20% code)
