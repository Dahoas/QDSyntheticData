# [ARB: Advanced Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2307.13692)

## Summary

Constructs an LLM benchmark of multiple-choice style and open-ended style acaedmic questions. Additional emphasis is put into investigating the ability of LLMs to evaluate open-ended solutions via rubric-based scoring.

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms

List of algorithm categories the proposed method falls under. This will take some time to formalize. Example categories include i.i.d sampling, few-shot i.i.d sampling, MAP Elites, 

- LLM as a judge (with rubric)

## Benchmarks

Tasks evaluated.

- ARB

## Metric Results

- How is quality measured: N/A
- How is diversity measured: By topic/difficulty

## Paper Tags

Tag paper with relevant categories. See [here](https://github.com/Dahoas/QDSyntheticData/blob/main/papers/categories.json) for list of all category tags.

1. DATASET
2. JUDGE
3. REASONING
