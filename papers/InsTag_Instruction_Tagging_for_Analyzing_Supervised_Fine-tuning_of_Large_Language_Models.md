# [#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models](https://arxiv.org/abs/2308.07074)

## Summary

They propose InsTag, a method called to measure dataset diversity and complexity by looking at ChatGPT-generated prompt tags.

First they use ChatGPT to tag instructions by intentions, rather than generalized, coarse-grained classes in an open-ended fashion.
ChatGPT isn't completely reliable so they need to normalise the tags. They note that there are three types of noisy labels:
1. Lexical Noise: ChatGPT not adhering to the output format. This can be cleaned through the use of stemming
2. Uncontrolled Granularity: Overly specific tags. They solve this by aggregating similar (as judged by a BERT model) tags into one
3. Spurious Correlations: Tags that often appear together. Such tags are also merged
They also apply frequency filtering, i.e. remove tags that occur too sparsely

Using these tags they consider dataset diversity and complexity, which they define as:
- Diversity is used to access the range of intentions and semantics covered by queries in a dataset. According to the tagging results, a dataset is considered more diverse if it covers more individual tags. The attribute is quantified as the unique tag coverage rate for the overall tag set.
- Complexity aims to measure the number of intentions and semantics complicating queries. We assume a more complex query would be assigned more tags. The attribute is quantified as the average tag number assigned to queries in a dataset.

They show that performance increases as a function of both of these. With this they filter down WizardLM(Alpaca), WizardLM(ShareGPT), UltraChat and ShareGPT to get a maximally diverse and complex dataset.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

- InsTag
- Complexity-first Diverse Sampling: Creates a complex dataset that contains all tags

## Benchmarks

- AlpacaEval
- MT-Bench

## Metric Results

- Their dataset outperforms other datasets that are much larger on the above benchmarks

## Paper Tags

- COMPLEXITY
- DIVERSITY

## Other comments

I wonder whether some of this performance is due to model-inbreeding: The data is generated by GPT, then filter by using GPT-generated tags and then judged by GPT. I think it'd be interesting to see whether measures of diversity generalise across models.
