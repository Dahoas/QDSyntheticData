# [Iterative Reasoning Preference Optimization](https://arxiv.org/abs/2404.19733)

## Summary

Combines SFT and DPO to notably boost pass@1 performance of LLMs on reasoning tasks. Far outperforms either SFT or DPO independently.

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms

- Reasoning Preference Optimization (RPO)

## Benchmarks

Tasks evaluated.

- GSM8K
- MATH

## Metric Results

- Quality measured via final answer correctness

## Paper Tags

Tag paper with relevant categories. See [here](https://github.com/Dahoas/QDSyntheticData/blob/main/papers/categories.json) for list of all category tags.

1. Reasoning
2. JUDGE
3. SYNTH

## Questions

How does SFT + DPO affect LLM output diversity?
