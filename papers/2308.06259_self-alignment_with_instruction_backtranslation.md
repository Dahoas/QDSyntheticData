# [Self-Alignment with Instruction Backtranslation](https://arxiv.org/abs/2308.06259)

## Summary

The paper "Self-Alignment with Instruction Backtranslation" proposes a method for generating high-quality instruction-following data using a language model itself, without relying on human annotations or distillation from more powerful models.
The approach, called instruction backtranslation, starts with a small amount of seed instruction data and a large web corpus.
It iteratively uses the language model to generate instructions for the web pages (self-augmentation) and select the highest quality instruction-output pairs to finetune the model on (self-curation).
The resulting model, Humpback, outperforms other non-distilled instruction-following models on the Alpaca leaderboard as of the time of the paper.

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms

List of algorithm categories the proposed method falls under. This will take some time to formalize. Example categories include i.i.d sampling, few-shot i.i.d sampling, MAP Elites, 

- Rephrasing Data
- Self Scoring

## Benchmarks

Tasks evaluated.

- AlpacaEval
- Human Preference

## Metric Results

- How is quality measured? Model is prompted to score self instruction outputs on a 5 point scale for quality
- How is diversity measured? N/A - Uses web corpus to sidestep this issue
- Fine-tuning results? Better than other non-GPT4 distillation methods as of the time the paper is written for Alpaca Eval

## Other comments

One of the only papers I've seen that train a model for creating rephrase/instruction data, seems underexplored
