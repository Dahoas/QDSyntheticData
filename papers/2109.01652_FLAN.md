# [FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS](https://arxiv.org/abs/2109.01652)

ID: 194

## Summary

1. The paper proposes Instruction Tuning as a way to improve the zero shot capabilities of (decoder-only) Language models.
Instruction tuning teaches models to be better instruction followers which is more than just completing sentences, and hence consequently, it doesn't do much for tasks where instructions are redundant eg. common sense reasoning.

2. Instruction tuning datasets are created by leveraging the existing open source datasets using specific standard templates that are different for each task and that convert the existing datasets into instruction-response pairs. A total of 62 such datasets are grouped into 12 task clusters.

3. Pre-trained LaMDA-PT is instruction tuned on datasets from all clusters but for the one whose datasets the instruction tuned model is evaluated on. This leave-one-train-on-the-rest thing is used to train several ckpts (collectively called FLAN) that are evaluated on daatsets from the unseen clusters.

## Relevance to survey topic (1-5)

Relevance: 1
Because IMO, the central premise of the paper is that instruction tuning teaches models to be better instruction followers which is more than just completing sentences.
But, it's not much relevant to what we're interested in: methods to generate synthetic data, and their effect on data quality and diversity.

## Algorithms

- SFT

## Benchmarks

- NLI: ANLI, RTE, CB, MNLI, QNLI, SNLI, WNLI
- commonsense: CoPA, HellaSwag, PiQA, StoryCloze
- sentiment: IMDB, Sent140, SST-2, Yelp
- Paraphrase: QQP, MRPC, PAWS, STS-B
- Closed-book QA: NQ, ARC, TQA
- Struct to text: DART, CommonGen, E2ENLG, WEBNLG
- Translation: ParaCrawl EN/ES, ParaCrawl EN/DE, ParaCrawl EN/FR, WMT-16 EN/CS, WMT-16 EN/DE, WMT-16 EN/FI, WMT-16 EN/RO, WMT-16 EN/RU, WMT-16 EN/TR
- Reading comp.: DROP, BoolQ, MultiRC, OBQA, SQuAD
- Read. comp. w/ commonsense: CosmosQA, ReCoRD
- Coreference: Winogrande, DPR, WSC273
- Misc.: QuAC, CoQA, WIC, TREC, CoLA, Math, Fix Punctuation (NLG)
- Summarization: AG News, AESLC, CNN-DM, Gigaword, Multi-News, Newsroom, Opin-Abs: iDebate, Opin-Abs: Movie, SamSum, Wiki Lingua EN, XSum

## Metric Results

- How is quality measured?
  - N/A
- How is diversity measured?
  - A slightly relevant strategy from the paper: "To balance the different sizes of datasets, we limit the number of training examples per dataset to 30k and follow the examples-proportional mixing scheme (Raffel et al., 2020) with a mixing rate maximum of 3k."
- Fine-tuning results?
  - FLAN outperforms zero-shot (20 of 25 datasets) and few-shot GPT-3 (10 datasets). 
  - Similarly, zero-shot FLAN outperforms zero-shot GLaM on 13 of 19 available datasets and one-shot GLaM on 11 of 19 datasets.
  -  Instruction tuning significantly improves LaMDA-PT (the pre-trained baseline) on most datasets.

- How does dataset quality, diversity, and scale affect model generalization?
  - Don't think there's sufficient work in the paper that addresses this.
- How do quality, diversity, and data scale relate?
  - N/A

## Paper Tags

- DATASET (https://github.com/google-research/flan)
- REASONING (In the sense that common reasoning benchmarks are evaluated)


## Other comments
No other comments.