# [MAmmoTH2: Scaling Instructions from the Web](https://arxiv.org/abs/2405.03548)

ID: 

## Summary

This paper performs a Rephrase-the-Web style synthetic instruction dataset curation via locating relevant instruction-like data on the web and extracting instruction data then further refining it with Mixtral-8x7B. They focus on Mathematics, STEM, and other common benchmarks like BBH and achieve strong performance when fine-tuning Mistral-7B and Mixtral-8x7B.

## Relevance to Survey Topic

Relevance: 5

## Algorithms

- Recall relevant data from the web: begin with 100k datapoints crawled from "educational websites" (Stackexchange?). Train a FastText classifier for collecting the most relevant domains, also using GPT-4 to filter out which seem most promising, and iterate twice.
- QA Pair extraction: Qwen-72B is used to locate and extract questions and answers in this set of recalled documents. 
- Refinement with LMs: the extracted QA pairs are then passed through Mixtral 8x22B and Qwen-72B to improve formatting and add CoT reasoning traces. 

## Benchmarks

- MMLU-STEM
- GSM8k
- GPQA
- BBH
- TheoremQA
- MATH
- MBPP
- AlpacaEval 2
- Arena Hard

## Metric Results

Quality is measured by ablating benchmark performance at several small scales, comparing downstream task performance when using pre- and post- LM-refined QA data.

Diversity is targeted by using real seed data and real data to extract QA / instruction information from, but not directly measured. It is also incentivized by using two different LMs (Mixtral 8x22B and Qwen-72B) to refine the data.

Primarily, performance is reported via downstream accuracy on the above benchmarks.

## Paper Tags

1. SYNTH
2. DATASET
3. REASONING

## Other Comments

They get the best results when mixing this data in with existing instruction datasets: OpenHermes 2.5, Code-Feedback, and Math-Plus. The datasets include GPT-4 generated synthetic data and use the benchmarks tested on like GSM/MATH for seed data. These datasets also include MetaMathQA. 

Decontamination using 10-gram overlap is also done, which should be pretty effective at removing exact dupes of solutions, but see above for concerns about overfitting which aren't captured by 10-gram dedupe.

A subset of the dataset is public at https://huggingface.co/datasets/TIGER-Lab/WebInstructSub . 
