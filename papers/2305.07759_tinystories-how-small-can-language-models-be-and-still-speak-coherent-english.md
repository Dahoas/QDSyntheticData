[TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)

## Summary

What are the minimal requirements for a language model to achieve this ability?

TinyStories is designed to capture the essence of natural language, while reducing its breadth and diversity.

Phi-0:
- 10M parameters
- 256 hidden
- 1 Layer

## Algorithm

Data Generation: Instruct the models to produce content that only uses vocabulary that a typical 3-year-old child would understand in the format of short stories in english language.

1. Select 1500 words (words, verbs, adjectives)
1. Sample triplets: (word, verb, adjective)
1. generate a story using the triplet

add features:
- dialogue
- plot twist
- bad ending
- morale value
- foreshadowing
- conflict 

"""
Write a short story (3-5 paragraphs) which only uses very simple words that a 3 year old child would likely understand. The story should use the verb 'decorate', the noun 'thunder' and the adjective 'ancient'. The story should have the following features: the story should contain at least one dialogue, the story has a bad ending. Remember to only use simple words!
"""

GPT Eval

We provide the model with a story's beginning, taken from a manually-prepared dataset consisting of around 50 prompts, generate a completion using the model, and provide the story's beginning together with the model's completion to GPT-4, asking it to grade the completion assignment in terms of grammar, creativity, and its consistency with the beginning of the story.

Trained model of 28M parameters generate 10 completions with temperature 1



- Consistent / InConsistent 
- Diverse / Repetitive
- Meaningful / Non Sensical

Maintain a clear topic or logical structure across paragraphs.

## Metrics

Rouge-K precision score 

## Comments

Small models are more interpretable.

- Model depth is more important for keeping consistent with the content than for generating syntactically correct language.
- Consistency and creativity only emerges at larger scale 
- Consistenty Emerges with a hidden size of 128 or larger. 
- Instruction following is achieved only after 2 layers.

- The quality of completion slightly decays when increasing the temperature.

Three level of memorization:
- Exact memorization: checking This can be detected checking the similarity or the hash of the generated story with the stories in the dataset.
- Simple Template Matching: This can be detected and prevented by measuring the overlap of words and n-grams between the generated story and the stories in the dataset.
- Complex Template Matching: Very hard to detect.

## References

- https://huggingface.co/datasets/roneneldan/TinyStories

## TAGS

- DATASET