# [TinyGSM: achieving >80% on GSM8k with small language models](https://arxiv.org/abs/2312.09241)

ID: 140

## Summary

Studies how high-quality datasets may be the key for small language models to acquire mathematical reasoning. 
Introduces TinyGSM, a synthetic dataset of 12.3M grade school math problems paired with Python solutions, generated by GPT-3.5-turbo.
After finetuning on TinyGSM, they find that a duo of a 1.3B generation model and a 1.3B verifier model can achieve 81.5% accuracy on GSM, 
Impressive results as only models breaking 80% barrier on the GSM8K benchmark are 34B parameters or more.
They beat the performance of the GPT-3.5 "teacher" model that generated the data (77.4%). 
The two key components are 1) the high-quality dataset TinyGSM, and 2) the use of a verifier, which selects the final outputs from multiple candidate generations.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

List of algorithm categories the proposed method falls under. This will take some time to formalize. Example categories include i.i.d sampling, few-shot i.i.d sampling, MAP Elites, 

- Few shot verification? 

## Benchmarks and results

They finetune the Phi-1.5 125M, 350M and 1.3B models on TinyGSM. The finetuning
phase takes up to 20k steps in total.

They demonstrate finetuning the Phi-1.5 1.3B model on TinyGSM’s which improves its accuracy from 44.6% to 68.2% (before the use of verifiers) on the GSM8K test set. 
The smallest 125M model can also achieve 63.1% after finetuning on TinyGSM.
With verifiers, the 1.3B model achieves 81.5% accuracy on GSM8K. 

For verifier training, they identify data diversity as a crucial element for a verifier’s success, and find that the
scaling of the verifier may be more effective than scaling of the generator;
while scaling up from a 125M generator to a 1.3B generator only gives a 5.1% increase in performance (Table 1), 
scaling up the verifier from 125M to 1.3B leads to a 7.2% performance boost. 

## Quality diversity details

To encourage diversity, they use temperature sampling and specify in the prompt to encourage the problem
variants to be grammatically diverse and contain multiple steps. 
To ensure the quality of the synthetic data in TinyGSM, they filter out problems that are too short or
do not contain numbers, as well as code solutions which are not executable. 
They do not check for the correctness of the question or the generated solutions.

- How is quality measured? Filter to working python code and verifier score. 
- How is diversity measured? Little discussion on diversity. 

## Paper Tags

1. SYNTH
2. DATASET
3. REASONING

## Other comments

Dataset is much larger than GSM8K as it consists of 12.3M problems (1.8B tokens). 

They prompt GPT-3.5-turbo to generate problem variants similar to a given question (but not the solution) randomly sampled from the GSM8K training set. 
To enhance robustness, they also generated synthetic problems whose questions contain irrelevant information. 

### the verifier 

They use a separate verifier for selecting candidate generations. For each base generation LM, they trained a verifier to predict whether a generation
is a correct solution to the given question. During inference, they generate multiple candidate generations using
temperature sampling and select the one with the highest verifier score.

Its training data consists of their small LM’s generations on the labelled GSM8K, paired with the binary labels indicating whether a generation leads to the correct numerical answer. 

They found the best of multiple generations significantly outperforms a single generation. 
These generations could be low-temperature generations from different checkpoints of a single run, where taking the best out of generations
from 5 checkpoints of (an early version of) the 350M model reaches 75% accuracy. The generations could also be
from high-temperature generations based on a single checkpoint; for instance, the pass@32 accuracy of the 1.3B
model is 94% (pass@k is accuracy by taking the best of k generations).

They find greater success in generating a solution that passes verification in leveraging multiple generations of the LM. 

