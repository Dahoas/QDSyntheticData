# [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2210.10760)

ID: #143

## Summary

Synthetic setting: 
This paper uses a synthetic setup (due to the expense of human reward data) to study reward model overoptimisation in the context of LLMs.
Observations, or prompts are drawn from a broad range of natural language instructions describing different language model tasks. Then, a learned RM is used to provide the reward signal for the response. 
In their synthetic setting, they use a "Gold RM" (trained using real human preference data) as ground truth. So the proxy RM is a proxy for the Gold RM ground truth process generating the labels (instead of typically approximating human preferences). 

As a reward model (RM) is an imperfect proxy, overoptimisation can hinder ground truth performance, à la Goodhart’s law.
A ﬁxed “gold-standard” reward model plays the role of humans, providing labels used to train a proxy reward model. 
They study how this gold reward model score changes as they optimise against the proxy reward model using RL and rejection sampling. 

## Relevance to survey topic (1-5)

Relevance: 3 - uses synthetic data for ease, it's not the focus of the paper. 

## Algorithms

- Fits functional forms to find scaling laws, they tried a few functional forms (Appendix B). 

## Metric Results

They find a functional relationship for each type of optimisation varying smoothly for varying reward model parameter count. 
Also study the relationship with the size of the reward model dataset, the number of reward model and policy parameters, and the coefﬁcient of the KL penalty. 
Finds RL is slower than rejection sampling at optimisation and that KL distance is an inadequate metric for quantity of (over)optimisation. 

## Paper Tags

1. SYNTH
