# [Simple synthetic data reduces sycophancy in large language models](https://arxiv.org/abs/2308.03958)

ID: 324

## Summary

The paper finds that scaling and instruction tuning can increase sycophancy in language models. It presents a simple synthetic data intervention that uses publicly-available NLP datasets to teach a model that a statement’s truthfulness is independent of a given user’s opinion. to reduce sycophancy. Fine-tuning on the generated data demonstrates a successful reduction in sycophancy.

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms

Three-stage process:

1. **Claim creation**: using input–label pairs from existing NLP tasks.

2. **Opinion generation**: adding a user opinion that agrees or disagrees with the claim.

3. **Prompt generation**: The claim and the associated opinion are then inserted into a fixed template to generate a prompt  

## Benchmarks

- Three Sycophancy Tasks from Perez et al. (2022):
  
  - Natural language processing survey questions (NLP).
  - Philosophy survey questions (PHIL).
  - Political typology quiz questions (POLI).

- A proposed dataset of 2.5k simple addition statements that are objectively incorrect.

## Metric Results

- **On syncophancy tasks**, all model sizes show a considerable reduction in sycophancy (with the largest reduction being 10% less)

- **On simple addition statements**, larger models showed improvements while smaller models showed a change in behavior to always agreeing with the incorrect statements.

## Paper Tags

Tag paper with relevant categories. See [here](https://github.com/Dahoas/QDSyntheticData/blob/main/papers/categories.json) for list of all category tags.

1. DVFT
2. SYNTH
3. REASONING
4. MOTIV

## Relevant related work

- Tomer Wullach, Amir Adler, and Einat Minkov. **Fight fire with fire: Fine-tuning hate detectors using large samples of generated hate speech**. [Link](https://arxiv.org/abs/2109.00591).

## Other comments

- The generated prompts follow the format introduced in Perez et al. (2022), it is unclear whether these results generalize to other formats.
- The introduced addition statements dataset contains only false addition statements.
