# [OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset](https://arxiv.org/abs/2402.10176)
## Summary

OpenMathInstruct-1, is a synthetic math instruction tuning dataset with 1.8M problem-solution pairs.
The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, using Mixtral8x7B model.
Source code and dataset are fully opensource.

## Relevance to survey topic (1-5)

5

## Algorithms

The paper use the metric training set coverage (TSC) to compare the models, where TSC measures the number of training problems for which any of the generated solutions leads to the ground truth answer (pass@k).
OpenMathInstruct-1 has a training set coverage of 93% for MATH and 99.9% for GSM8K.

The best model, OpenMath-CodeLlama-70B, achieves a score of 84.6% on GSM8K and 50.7% on MATH.

Use few-shot prompting (K=5), with temperature=1.0 and top_p=0.95. A key ingredient of this pipeline is the few-shot prompt examples. 128 solutions per problem (GSM8k) and 224 for MATH. 


MATH has subjects (7):
- algebra
- geometry
- intermediate algebra
- number theory
- prealgebra
- precalculus
- probability

For each of the seven prompts, we sample 32 solutions per problem and combine the data generated with all the prompts, which is equivalent to 32 x 7 = 224 solutions per problem.

## Benchmarks

The model performs an absolute 13.3% better when using code for answering questions in comparison to when not using it.
- Text-based 32.0%
- Code + Text 45.3%

The model scores 72.4% on Level 1 problems and only 16.3% on Level 5
74% of the errors in code solutions are due to reasoning error and the remaining 26% attributable to execution-related issues.

## Metric Results

- How is quality measured?
- How is diversity measured?
- Fine-tuning results?

## Paper Tags

- SYNTH
- DATASET


## Other comments

1. Brute force approach of sampling several solutions per problem only scales logarithmically.
2. Targeted Solution Generation, few shot prompts focused for each of the subject in the MATH datasets and merge the synthesized solutions. This leads only to a marginal gain in TSC.
3. Using the text solutions in the prompt to generate code-interpreter style solutions leads to the best improvement in terms of coverage. One caveat is that the system can generate trivial solutions such as print(ANSWER) or The answer is ANSWER where the ANSWER is copied from the text solution in the prompt. The LLM is very creative at cheating and a manual filtering solution is not practical. The solution is to use masked text solution were all numbers in intermediate computations are replaced with symbols.

OpenMathInstruct is obtained by merging and deduplicating the problem-solution pairs resulting from the described prompt strategies.

## Post Processing
- Syntactic Noisy solutions: mostly not abiding the induced code-block format are filtered out.
- Semantically Noisy solutions: solutions with flawed reasoning, very few they are harder to detect.

## Data Selection
- Downsampling: naive vs fair/round-robin
experiments show that fair outperforms naive downsampling.
- Code Preferential: 
    1. mayority code: if $N_{code} > N_{text}$ remove all the text solution
    1. any-code if $N_{code} > 0$ remove all the text solution

## FineTuning
- 512k from GSM8K + 512k from MATH using the any-code filtering.

## Evaluation
- ZeroShoT
- Greedy Decoding
- Self-Consistency / Majority Voting (temperature=0.7 lower than generation) K = 50
