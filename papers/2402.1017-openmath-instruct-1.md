# [OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset](https://arxiv.org/abs/2402.101760)
## Summary

OpenMathInstruct-1, is a synthetic math instruction tuning dataset with 1.8M problem-solution pairs.
The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, using Mixtral8x7B model.
Source code and dataset are fully opensource.

## Relevance to survey topic (1-5)

5

## Algorithms

The paper use the metric training set coverage (TSC) to compare the models, where TSC measures the number of training problems for which any of the generated solutions leads to the ground truth answer (pass@k).
OpenMathInstruct-1 has a training set coverage of 93% for MATH and 99.9% for GSM8K.

The best model, OpenMath-CodeLlama-70B, achieves a score of 84.6% on GSM8K and 50.7% on MATH.

Use few-shot prompting (K=5), with temperature=1.0 and top_p=0.95. A key ingredient of this pipeline is the few-shot prompt examples. 128 solutions per problem (GSM8k) and 224 for MATH. 


MATH has subjects (7):
- algebra
- geometry
- intermediate algebra
- number theory
- prealgebra
- precalculus
- probability

For each of the seven prompts, we sample 32 solutions per problem and combine the data generated with all the prompts, which is equivalent to 32 x 7 = 224 solutions per problem.

## Benchmarks

Tasks evaluated.

- Benchmark 1
- Benchmark 2
- Benchmark 3

## Metric Results

- How is quality measured?
- How is diversity measured?
- Fine-tuning results?

## Paper Tags

Tag paper with relevant categories. See [here](https://github.com/Dahoas/QDSyntheticData/blob/main/papers/categories.json) for list of all category tags.

1. TAG 1
2. TAG 2
3. TAG 3

## Relevant related work

Add them to the spreadsheet. No need to link them in the report.

## Other comments

1. Brute force approach of sampling several solutions per problem only scales logarithmically.
2. Targeted Solution Generation, few shot prompts focused for each of the subject in the MATH datasets and merge the synthesized solutions. This leads only to a marginal gain in TSC.
3. Using the text solutions in the prompt to generate code-interpreter style solutions leads to the best improvement in terms of coverage. One caveat is that the system can generate trivial solutions such as print(ANSWER) or The answer is ANSWER where the ANSWER is copied from the text solution in the prompt. The LLM is very creative at cheating and a manual filtering solution is not practical. The solution is to use masked text solution were all numbers in intermediate computations are replaced with symbols.

OpenMathInstruct is obtained by merging and deduplicating the problem-solution pairs resulting from the described prompt strategies.

## Post Processing
- Syntactic Noisy solutions: mostly not abiding the induced code-block format are filtered out.
- Semantically Noisy solutions: solutions with flawed reasoning, very few they are harder to detect.

## Data Selection
- Downsampling: naive vs fair/round-robin
experiments show that fair outperforms naive downsampling.
- Code Preferential: 
    1. mayority code: if $N_{code} > N_{text}$ remove all the text solution
    1. any-code if $N_{code} > 0$ remove all the text solution

## FineTuning
- 512k from GSM8K + 512k from MATH using the any-code filtering.

## Evaluation
- ZeroShoT
- Greedy Decoding
- Self-Consistency / Majority Voting (temperature=0.7 lower than generation) K = 50

## Questions

- Could the diversity of mathematical topics in MATH be a reason for the low training set coverage with a single 5-shot prompt?

- Can we make the solution generation pipeline more efficient?


- 512k examples from GSM8K that has only 8k samples seems strange. 