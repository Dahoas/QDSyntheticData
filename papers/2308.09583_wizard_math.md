# [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583)

ID: 238

## Summary

1. The paper proposes RLEIF (Reinforcement Learning from Evol-Instruct Feedback) as a way to improve the mathematical reasoning abilities of Llama-2.

2. SFT: WizardLM 70B is used to re-generate answers to GSM8k and MATH in a step-by-step format, instruction-solution pairs with correct answers are filtered, and Llama 2 is fine-tuned on this data + 1.5k open-domain examples from wizardLM's train set.
The goal of this step is just to teach the step-by-step answering format to the model.

3. IRM: Evol-Instruct from the WizardLM paper is used to evolve datasets GSM8k + MATH on two lines: Downward evolution (makes questions easier), and upward (makes questions harder); ChatGPT and Wizard-e being the models used.

Wizard-e is further used to rank the evolved instructions corresponding to each original instruction - This ranking data is used to train an IRM (instruction reward model) to predict the quality of evolved instructions.

4. PRM: ChatGPT is used to obtain the step level correctness of the data generated by the SFT'd Llama 2. This data is used to train a PRM (process reward model) to generate step level rewards.

Both the IRM and the PRM are trained from llama-2.

5. Active evol-instruct and PPO training: A final reward r = r_i (instruction reward from the IRM) * r_a (answer reward from the PRM) is used for this step.

## Relevance to survey topic (1-5)

Relevance: 5


## Algorithms

- PPO
- SFT
- Reward modelling (probably pair-wise preference for IRM and process supervision for PRM)

## Benchmarks

- GSM8K
- MATH

## Metric Results

- How is quality measured?
  - An IRM is trained to predict the quality of the synthetically generated (evolved) instructions. This IRM in turn is trained using the synthetic data itself.
- How is diversity measured?
  - Didn't find any measure. Just that the two lines of evolution are intended to produce a diverse variety of synthetic instructions. Another aspect of diversity is present in the SFT training set which includes 1.5k open domain exmaples in addition to just the math examples. But these 1.5k examples are probably not synthetic so not too relevant.
- Fine-tuning results?
  - WizardMath 70B outperforms all open source models on both GSM8K and MATH while it outperforms some closed source models on MATH. Very elaborate results present in table 1 of the paper.
- How does dataset quality, diversity, and scale affect model generalization?
  - Don't think there's sufficient work in the paper that addresses this.
- How do quality, diversity, and data scale relate?
  - N/A

## Paper Tags

- SYNTH
- REASONING
- JUDGE
- QUALITY (by means of the IRM being able to predict the quality of the synthetic instructions)
- DIVERSITY (only a slight emphasis)
- EVO (not sure)

## Other comments
No other comments.