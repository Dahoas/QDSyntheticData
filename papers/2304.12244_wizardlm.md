# [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://openreview.net/forum?id=CfXh93NDgH)

ID: 236

## Summary

Introduces a new, scalable method of high-quality, diverse synthetic data generation with LLMs for instruction data. Applies Evol-Instruct, a method to generate new instructions of increasing complexity (interesting, high-quality) and diversity by prompting LLMs to re-write prompts containing instructions and inputs to create more complex tasks (e.g. in-depth evolving), or inspire new kinds of instruction tasks (e.g. in-breadth evolving). One of the first works to apply an important aspect of open-endedness (bottom of paragraph of page 2 in [ref](https://openreview.net/forum?id=Hel8XD-8Z9)) to synthetic data generation, through the continual generation of increasing complexity and novelty of data in the pipeline for generating a population/corpus of instruction finetuning instances.

## Relevance to survey topic (1-5)

Relevance: 5 (to at least one central theme)

## Algorithms

- Instructed Mutation (a kind of Evolution through Large Models, or ELM)
- Population/Archive-based search with minimal set of quality filters

## Benchmarks

- Human evaluation of WizardLM model responses on Evol-Instruct testset, and high-difficulty set (assessed by LLM-generated Likert score), compared to other models.
- GPT-4-as-a-judge (Vicuna setup)

## Metric Results

- How is quality measured?
  - Complexity and difficulty of instructions, as assessed by GPT-4's 1-10 Likert rating
- How is diversity measured?
  - t-SNE and k-means clustering of BERT embeddings to qualitatively display breadth of instruction data, compared to ShareGPT and Alpaca datasets, but no quantitative results
- Fine-tuning results?
  - Humans prefer WizardLM vs. GPT-4 on "difficult" task responses, in both human and automatic judging, 
- How does dataset quality, diversity, and scale affect model generalization?
  - Quantity: "proportion" of math data in seed set (4.5) likely influences math performance, larger evolved dataset size used improves performance across various benchmarks
- How do quality, diversity, and data scale relate?
  - N/A

## Paper Tags

- SYNTH
- EVO
- JUDGE
- DIVERSITY (somewhat)

## Other comments

- Quality filters for generated instructions include LLM-judged assessment on evolved task "information gain", minimal criterion where the LLM must be able to give a response >80 words without saying "sorry", response by LLM is not only punctuations/stop words, instruction must not copy words from the evolving prompt.
- arXiv pre-print not up-to-date with ICLR version
