# [A Survey on Data Selection for Language Models](https://arxiv.org/abs/2402.16827)


## Summary

Machine learning models are a method for modeling statistical patterns in data and, from the probabilistic viewpoint, the optimal dataset is that which most closely matches the distribution under which the model will be evaluated.

Dataselection GOALS:
- optimize model performance
- reduce costs
- ensure integrity (remove test data from train data)
- remove bias, toxicity, harmfulness

Strategy:
Create a common framework where all the data selection methods define:
- a utilty function that determines the utilty of data
- a selection mechanism that determines how to use a data point based on its utility

This allows to define a TAXONOMY of the existing methods.
- Pre-Training
- Instruction-Tuning
- Alignment
- In-Context Learning
- Task-specific Fine-Tuning

Definitions:
- Token: one or more bytes, depends on the tokenizer.
- Data Point: ordered collections of tokens.
- Data Point Characteristics: number of measures that you can apply to a data point.
- Dataset: collection of datapoints
- Dataset distribution: The distribution of (?). This distribution has a significant impact on the final capabilities and properties of a model trained on it.
    - a dense region of the data distribution generally suggests that a model will perform well on unseen data from that region.
    - increasing the density of data around desirable regions while reducing density around undesirable regions generally leads to a model that performs well in the desired settings

Possible characteristic of a data point:
- number of characters
- number of alphabetic characters
- distributed representation: embedding of the data point


Possible utility functions:
- number of characters > 10
- is it a wikipedia article

Possible selection functions:
- selection sensitivity: only include if utility(x) > *threshold*


Distribution diversification methods often remove data points that are similar in some representation space.

filtering: binary value
mixing: selecting subset of the dataset 

- classifier-based quality filtering
    np.random.pareto(alpha) > 1 - document_score: still include some documents that are out of distribution with respect to the reference corpora
    Sampling (without replacement) according to the importance weights can be done efficiently with the Gumbel top-k trick, which perturbs the unnormalized log-importance weights with Gumbel noise before selecting the top k
- perplexity-based quality filtering


## Deduplication
- exact matching: hash, urls
- approximate matching: model-based


## Relevance to survey topic (1-5)

Relevance: ?

## Algorithms

List of algorithm categories the proposed method falls under. This will take some time to formalize. Example categories include i.i.d sampling, few-shot i.i.d sampling, MAP Elites, 

- Algorithm 1
- Algorithm 2
- Algorithm 3

## Benchmarks

Tasks evaluated.

- Benchmark 1
- Benchmark 2
- Benchmark 3

## Metric Results

- How is quality measured?
- How is diversity measured?
- Fine-tuning results?

## Paper Tags

Tag paper with relevant categories. See [here](https://github.com/Dahoas/QDSyntheticData/blob/main/papers/categories.json) for list of all category tags.

1. TAG 1
2. TAG 2
3. TAG 3

## Relevant related work

Add them to the spreadsheet. No need to link them in the report.

## Other comments

Anything else you found interesting/noteworthy.

## Questions

Questions you had about the paper
