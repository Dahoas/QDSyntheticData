# [Scaling_Relationship_on_Learning_Mathematical_Reasoning_with_Large_Language_Models](https://arxiv.org/abs/2308.01825)

## Summary

Examines the scaling relationship between pretraining loss and ICL/SFT/RFT performance on reasoning tasks. Models with better pretraining loss experience diminishing returns for SFT. Finds RFT works best when samples are taken over multiple models to promote diversity.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

- RFT (Expert Iteration)

## Benchmarks

- GSM8K

## Metric Results

- Quality is measured via final answer correctness.
- Diversity is measured via order of arithmetic operations

## Paper Tags


1. SYNTH
2. DIVERSITY
