# [Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge](https://arxiv.org/abs/2407.19594)

ID: 335

## Summary

Meta uses LLMs to judge the judges of synthetic data pipelines! 
Beyond generating synthetic data and having a synthetic judge filter that data, this paper uses a third role of metajudge, whose task is to evaluate the model’s own judgements. 
While the judge evaluates the actor’s responses, the meta-judge evaluates the judge’s judgments (including rewards that it assigns) using "LLM-as-a-Meta-Judge", a mechanism similar to LLM-as-a-Judge. 
The meta-judge enables them to build training data containing preference pairs of judgements, in addition to the standard preferences between actor responses derived from the standard judge.

Paper show how it's possible to further improve the amount of synthetic data, such as by using LLMs to judge the judges of synthetic data pipelines.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

- DPO training and LLM-as-a-Meta-Judge

## Benchmarks and Metric Results

The technique seems to work well. 
They take a basic instruction-finetuned Llama-3-8B-Instruct model, and perform an iterative training process to try and bootstrap the 8B model into higher quality. 
In tests on AlpacaEval 2, they show good improvements: the base model goes from a 22.57% win rate against GPT4-Turbo to 39.45%. 
Similarly, when controlling for length it goes from a 22.9% winrate against Claude Opus to 39.4%. 


- AlpacaEval 2 - 22.9% to 39.4%
- Arena-Hard - 20.6% to 29.1% 

## Paper Tags

1. JUDGE

## Other comments

The method is an iterative training scheme that starts from a given seed LLM, which assumes all three roles. 
An iteration starts with the actor generating multiple response variations for each prompt. 
This is followed by the judge evaluating each response using an LLM-as-a-Judge prompt and generating a judgement that contains a score. 
This score then allows them to build preference pairs of responses for training the actor. 
For training the judge, they pick a single response and let the meta-judge compare two of its judgement variations 
generated by the judge to determine which one is better using an LLM-as-a-Meta-Judge prompt.  

So far, the technique only works for four iterations, where it seems like it could lead to reduced performance after that - 
but bear in mind a year or two ago, most synthetic data techniques only worked for one or two iterations before mode collapse, 
so the number of iterations we can do over time seems to be increasing.
