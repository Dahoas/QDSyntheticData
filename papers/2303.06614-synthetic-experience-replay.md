# [Synthetic Experience Replay](https://arxiv.org/abs/2303.06614)


## Summary

This work proposes: Synthetic Experience Replay (SYNTHER), a diffusion-based approach to flexibly upsample an agent's collected experience.
Experience replay, is a paradigm of RL where a dataset of past experiences is used to train a policy or value function.

SYNTHER enables online agents to train with a much higher update-to-data ratio than before, leading to a significant increase in sample efficiency.

### OFFLINE
In offline reinforcement learning, SYNTHER allows training from extremely small datasets, scaling up policy and value networks, and high levels of data compression.
The quality of the samples from the diffusion model is significantly better than with prior generative models such as VAEs or GANs.

### ONLINE
SYNTHER may be used to upsample an online agent's experiences by continually training the diffusion model on new experiences.
In online reinforcement learning, the additional data allows agents to use much higher update-to-data ratios leading to increased sample efficiency.
SYNTHER operates at the transition level.

# Questions
Why is SYNTHER better than explicit augmentation?

They compare the data generated by SYNTHER to that generated by the best-performing data augmentation approach, namely additive noise.
- How diverse is the data? To measure diversity, we measure the minimum L2 distance of each datapoint from the dataset, which allows us to see how far the upsampled data is from the original data.
- How accurate is the data for the purposes of learning policies? To measure the validity of the data, we follow Lu et al. and measure the MSE between the reward and next state proposed by SYNTHER with the true next state and reward defined by the simulator.

Remarks:
In general, synthetic data has not previously performed as well as real data on standard RL benchmarks.


Two key differences of this approach to our method compared to model based RL: SYNTHER synthesizes new experiences without the need to start from a real state and the generated experiences are distributed exactly according to the data, rather than subject to compounding errors due to modeling inaccuracy.


An immediate advantage of sampling data from a generative model is compression.

## Relevance to survey topic (1-5)

Relevance: ?

## Algorithms

List of algorithm categories the proposed method falls under. This will take some time to formalize. Example categories include i.i.d sampling, few-shot i.i.d sampling, MAP Elites, 

- Algorithm 1
- Algorithm 2
- Algorithm 3

## Benchmarks

Tasks evaluated.
- walker2d environment

- Benchmark 1
- Benchmark 2
- Benchmark 3

## Metric Results

- How is quality measured?
- How is diversity measured?
- Fine-tuning results?

## Relevant related work

Add them to the spreadsheet. No need to link them in the report.

## Other comments

Anything else you found interesting/noteworthy.

## Questions

Questions you had about the paper
