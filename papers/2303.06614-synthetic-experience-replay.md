# [Synthetic Experience Replay](https://arxiv.org/abs/2303.06614)


## Summary

This work proposes: Synthetic Experience Replay (SYNTHER), a diffusion-based approach to flexibly upsample an agent's collected experience.
Experience replay, is a paradigm of RL where a dataset of past experiences is used to train a policy or value function.

SYNTHER enables online agents to train with a much higher update-to-data ratio than before, leading to a significant increase in sample efficiency.

### OFFLINE
In offline reinforcement learning, SYNTHER allows training from extremely small datasets, scaling up policy and value networks, and high levels of data compression.
The quality of the samples from the diffusion model is significantly better than with prior generative models such as VAEs or GANs.

### ONLINE
SYNTHER may be used to upsample an online agent's experiences by continually training the diffusion model on new experiences.
In online reinforcement learning, the additional data allows agents to use much higher update-to-data ratios leading to increased sample efficiency.
SYNTHER operates at the transition level.

# Questions
Why is SYNTHER better than explicit augmentation?

They compare the data generated by SYNTHER to that generated by the best-performing data augmentation approach, namely additive noise.
- How diverse is the data? To measure diversity, we measure the minimum L2 distance of each datapoint from the dataset, which allows us to see how far the upsampled data is from the original data.
- How accurate is the data for the purposes of learning policies? To measure the validity of the data, we follow Lu et al. and measure the MSE between the reward and next state proposed by SYNTHER with the true next state and reward defined by the simulator.

Remarks:
In general, synthetic data has not previously performed as well as real data on standard RL benchmarks.


Two key differences of this approach to our method compared to model based RL: SYNTHER synthesizes new experiences without the need to start from a real state and the generated experiences are distributed exactly according to the data, rather than subject to compounding errors due to modeling inaccuracy.


An immediate advantage of sampling data from a generative model is compression.

## Relevance to survey topic (1-5)

Relevance: 2

## Algorithms

```
1: Input: real data ratio r ∈ [0, 1] 
2: Initialize: 
    - D_real = ∅ real replay buffer
    - π agent, 
    - D_synthetic = ∅ synthetic replay buffer, 
    - M diffusion model 
3: for t = 1, . . . , T do 
4:      Collect data with π in the environment and add them to D_real 
5:      Update diffusion model M with samples from D_real 
6:      Generate samples from M and add them to D_synthetic 
7:      Train π on samples from D_real ∪ D_synthetic mixed with ratio r 
8: end for
```

## TAGS

- SYNTH 

## Benchmarks

Tasks evaluated.
- walker2d environment

## Other comments

Show that the quality of the samples from the diffusion model is significantly better.

