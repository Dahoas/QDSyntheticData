# [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)


## Summary

phi-3-mini model:

- 3.8 B parameters
- 3.3 Trillion tokens
- 4K context
- 3072 hidden
- 32 heads
- 32 layers
- 32064 vocabulary
- chat template: <|user|>/n Question <|end|>/n <|assistant|>

A combination of LLM-based filtering of web data, and LLM-created synthetic data, enable performance in smaller language models that were typically seen only in much larger models.

Our training data of consists of heavily filtered web data (according to the "educational level") from various open internet sources, as well as synthetic LLM-generated data.

Pre-training is performed in two disjoint and sequential phases:

- Phase-1 comprises mostly of web sources aimed at teaching the model general knowledge and language understanding.
- Phase-2 merges even more heavily filtered webdata (a subset used in Phase-1) with some synthetic data that teach the model logical reasoning and various niche skills

POST TRAINING

- Supervised Fine Tuning
- Direct Preference Optimization

## Relevance to survey topic (1-5)

Relevance: 2

## Algorithms


## Benchmarks

- MMLU (5-Shot)
- HellaSwag (5-Shot) 
- ANLI (7-Shot)
- GSM-8K (0-Shot; CoT)
- MedQA (2-Shot) 
- AGIEval (0-Shot) 
- TriviaQA (5-Shot)
- Arc-C (10-Shot)
- Arc-E (10-Shot) 
- PIQA (5-Shot) 
- SociQA (5-Shot)
- BigBench-Hard (0-Shot)
- WinoGrande (5-Shot)
- OpenBookQA (10-Shot)
- BoolQ (0-Shot)
- CommonSenseQA (10-Shot)
- TruthfulQA (10-Shot) 
- HumanEval (0-Shot)
- MBPP (3-Shot) 
- GPQA (2-Shot; CoT) 
- MT Bench

## Paper Tags

1. MODEL

## Relevant related work

Add them to the spreadsheet. No need to link them in the report.

## Other comments

Anything else you found interesting/noteworthy.

## Questions

Questions you had about the paper
