# [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)

## Summary

SELF-INSTRUCT is a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations.

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

1. generating task instructions. 
    - Generate dataset from a set of 175 manually annotated SEED tasks.
    - For every step, we sample 8 task instructions from this pool as in-context examples.

1. determining if the instruction represents a classification task
    - We prompt the LM in a few-shot way to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks.

1. instance generation with either an input-first or output-first approach
    - Input-first Approach, where we can ask an LM to come up with the input fields first based on the instruction, and then produce the corresponding output.
    - 

1. filtering low-quality data: Finally, various heuristics are used to automatically filter low-quality or repeated instructions.

The iterative SELFINSTRUCT process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs.

SELF-INSTRUCT relies on a number of prompting templates in order to elicit the generation from language models.

## Benchmarks


## Metric Results


## Paper Tags

- DATASET

## Other comments


## Questions

