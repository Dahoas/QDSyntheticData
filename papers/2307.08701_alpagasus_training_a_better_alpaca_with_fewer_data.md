# [AlpaGasus: Training A Better Alpaca with Fewer Data](https://arxiv.org/abs/2307.08701)

## Summary

This paper uses ChatGPT to filter out low-quality data from the Alpaca dataset (used for instruction fine-tuning). This is done through a single human-written prompt. Then, it is shown that instruction fine-tuning on the filtered dataset (e.g., on 9k examples) produces a better instruction-tuned model compared to using the entire Alpaca dataset (52k examples).

## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

- Prompt an LLM (ChatGPT) to assign a 0-5 score to every example in the Alpaca dataset.

## Benchmarks

- Four instruction test sets (curated from Self-instruct, Vicuna, WizardLM, and Koala). GPT-4 is used as judge.
- MMLU, DROP, Humaneval, BBH.

## Metric Results

- Quality of instruction examples is estimated by a prompted LLM (ChatGPT).
- Diversity of instruction examples is briefly raised as a potential issue (in Section 6): the proposed approach filters out more coding-related data than average, and therefore the resulting instruction-tuned model is weaker at coding.


## Other comments

A simple approach to data filtering, with extended evaluation. The approach is effective (as measured by performance of the fine-tuned model), but its impact on data diversity is under-explored.
