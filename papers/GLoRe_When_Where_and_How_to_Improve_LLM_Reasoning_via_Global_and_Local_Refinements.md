# [GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements](https://arxiv.org/abs/2402.10963)

ID: Number of paper in list of papers (ordered by date of accepted pr)

## Summary

This paper compares outcome based reward models (ORMs) vs. synthetic process based reward models (SORMs) as method of improving LLM refinements on reasoning tasks. It finds taking the best of the initial draft, a local refinement guided by the SORM, and a global refinement to significantly outperform best of three samples from the base LLM.

## Relevance to survey topic (1-5)

Relevance: 3

## Algorithms

List of algorithm categories the proposed method falls under. This will take some time to formalize. Example categories include i.i.d sampling, few-shot i.i.d sampling, MAP Elites, 

- ORM
- PRM
- Refinement
- Expert Iteration

## Benchmarks

Tasks evaluated.

- GSM8K
- SVAMP

## Metric Results

- How is quality measured: N/A
- How is diversity measured: N/A

## Paper Tags

Tag paper with relevant categories. See [here](https://github.com/Dahoas/QDSyntheticData/blob/main/papers/categories.json) for list of all category tags.

1. SYNTH
2. JUDGE
3. REASONING